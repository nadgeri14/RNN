{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# The above are all the import necessary.                                                                             #\n",
    "# TF_CPP_MIN_LOG_LEVEL is used to control log onformation                                                             #\n",
    "# reset_default_graph used to reseat the graph if a one exists.                                                       #\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'arvix_abstracts.txt'\n",
    "HIDDEN_SIZE = 200\n",
    "BATCH_SIZE = 64\n",
    "NUM_STEPS = 50\n",
    "SKIP_STEP = 40\n",
    "TEMPRATURE = 0.7\n",
    "LR = 0.003\n",
    "LEN_GENERATED = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "#Hidden size is the length of GRU units                                                                               #\n",
    "#batch size is the size of batches in training set.                                                                   #\n",
    "#LR is the learning rate.                                                                                             #\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_encode(text, vocab):\n",
    "    return [vocab.index(x) + 1 for x in text if x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# vocab_encode is used to convert characters to numbers such that is is easy to learn.                                #\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_decode(array, vocab):\n",
    "    return ''.join([vocab[x - 1] for x in array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "#vocab_decide is used to convert number to a character                                                                #\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, vocab, window=NUM_STEPS, overlap=NUM_STEPS//2):\n",
    "    for text in open(filename):\n",
    "        text = vocab_encode(text, vocab)\n",
    "        for start in range(0, len(text) - window, overlap):\n",
    "            chunk = text[start: start + window]\n",
    "            chunk += [0] * (window - len(chunk)) # for zero padding in case it goes out of bounds\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Used to read data from the file and convert them into numbers, window size is the number of characters to be        #\n",
    "#included in the set that is being formed, overlap is the step size in the iteration. It should be less then whindow  #\n",
    "#size so as to capture the dependencies between the characters and words better.                                      #\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batch(stream, batch_size=BATCH_SIZE):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Used to create batches of trianing data. Returns an object that needs to be iterated at each step                   #\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn(seq, hidden_size=HIDDEN_SIZE):\n",
    "    cell = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "    in_state = tf.placeholder_with_default(\n",
    "            cell.zero_state(tf.shape(seq)[0], tf.float32), [None, hidden_size]) \n",
    "\n",
    "    # this line to calculate the real length of seq\n",
    "    # all seq are padded to be of the same length which is NUM_STEPS\n",
    "    #seq = tf.Print(seq,[seq, tf.shape(seq), tf.shape(seq)[0]],'seq--------------------->')\n",
    "    reduce_max = tf.reduce_max(tf.sign(seq), 2)\n",
    "    #print_reduce_max = tf.Print(reduce_max,[reduce_max,tf.shape(reduce_max)],'reduce_max------------>')\n",
    "    length = tf.reduce_sum(reduce_max, 1) \n",
    "    #temp_length = tf.Print(length, [length],'argmax(out) = ')\n",
    "    output, out_state = tf.nn.dynamic_rnn(cell, seq, length, in_state)\n",
    "    return output, in_state, out_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# create_rnn is used to create GRU cells for the model. We create the GRU cell with the given number of hiddem_size   #\n",
    "# We use the dynamic_rnn insted of static_rnn for improved performance.                                               #\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq, temp, vocab, hidden=HIDDEN_SIZE):\n",
    "    seq = tf.one_hot(seq, len(vocab))\n",
    "    output, in_state, out_state = create_rnn(seq, hidden)\n",
    "    # fully_connected is syntactic sugar for tf.matmul(w, output) + b\n",
    "    # it will create w and b for us\n",
    "    logits = tf.contrib.layers.fully_connected(output, len(vocab), None)\n",
    "    #seq = tf.Print(seq[:, 1:],[tf.shape(seq[:, 1:])],'seq------>')\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits[:, :-1], labels=seq[:, 1:]))\n",
    "    # sample the next character from Maxwell-Boltzmann Distribution with temperature temp\n",
    "    # it works equally well without tf.exp\n",
    "    sample = tf.multinomial(tf.exp(logits[:, -1] / temp), 1)[:, 0]\n",
    "\n",
    "\n",
    "    return loss, sample, in_state, out_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Create model used to create the rnn model and then connect it with fully connected model  for calulating the loss   #\n",
    "# and the next character that can appers in the model .                                                               #\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state):\n",
    "    saver = tf.train.Saver()\n",
    "    start = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter('graphs/gist', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/arvix/checkpoint'))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        iteration = global_step.eval()\n",
    "        for batch in read_batch(read_data(DATA_PATH, vocab)):\n",
    "            batch_loss, _ = sess.run([loss, optimizer], {seq: batch})\n",
    "            if (iteration + 1) % SKIP_STEP == 0:\n",
    "                print('Iter {}. \\n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "                online_inference(sess, vocab, seq, sample, temp, in_state, out_state)\n",
    "                start = time.time()\n",
    "                saver.save(sess, 'checkpoints/arvix/char-rnn', iteration)\n",
    "            iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Traning is where the actual training happens for the model is done. FileWriter are used to save the grpahs and the  #\n",
    "# checkpoint is used to store the current state of the model, which is then restored whn the model is trained again.  #\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_inference(sess, vocab, seq, sample, temp, in_state, out_state, seed='T'):\n",
    "    \"\"\" Generate sequence one character at a time, based on the previous character\n",
    "    \"\"\"\n",
    "    sentence = seed\n",
    "    state = None\n",
    "    for _ in range(LEN_GENERATED):\n",
    "        batch = [vocab_encode(sentence[-1], vocab)]\n",
    "        feed = {seq: batch, temp: TEMPRATURE}\n",
    "        # for the first decoder step, the state is None\n",
    "        if state is not None:\n",
    "            feed.update({in_state: state})\n",
    "        index, state = sess.run([sample, out_state], feed)\n",
    "        sentence += vocab_decode(index, vocab)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# is used to generate the sentence from the model.                                                                    #\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    vocab = (\n",
    "            \" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "            \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "    seq = tf.placeholder(tf.int32, [None, None])\n",
    "    temp = tf.placeholder(tf.float32)\n",
    "    loss, sample, in_state, out_state = create_model(seq, temp, vocab)\n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.AdamOptimizer(LR).minimize(loss, global_step=global_step)\n",
    "    training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Main of the program. Here the vocab is defined as all the letters (upper cases and lower cases) and the symbols that#\n",
    "# is possible in the language.                                                                                        #\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39. \n",
      "    Loss 9390.82226562. Time 10.6757669449\n",
      "TovH  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e\n",
      "Iter 79. \n",
      "    Loss 8181.9296875. Time 9.15825414658\n",
      "Th the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "Iter 119. \n",
      "    Loss 7406.81933594. Time 8.91854190826\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "Iter 159. \n",
      "    Loss 6931.84863281. Time 8.64301490784\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "Iter 199. \n",
      "    Loss 6370.15185547. Time 9.14766407013\n",
      "The serally the propesting and the convexter and athe the serally the propesting and the convexter and athe the serally the propesting and the convexter and athe the serally the propesting and the convexter and athe the serally the propesting and the convexter and athe the serally the propesting and \n",
      "Iter 239. \n",
      "    Loss 6109.95996094. Time 10.6557409763\n",
      "The the and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and a\n",
      "Iter 279. \n",
      "    Loss 5934.9921875. Time 8.18029212952\n",
      "The for the erformation the reconting a propestin a computation the reconting a propestin a computation the reconting a propestin a computation the reconting a propestin a computation the reconting a propestin a computation the reconting a propestin a computation the reconting a propestin a computati\n",
      "Iter 319. \n",
      "    Loss 6131.31933594. Time 7.86041593552\n",
      "The aralizetror and the erformance of the experimention of the experimention of the experimention of the experimention of the experimention of the experimention of the experimention of the experimention of the experimention of the experimention of the experimention of the experimention of the experim\n",
      "Iter 359. \n",
      "    Loss 5387.94287109. Time 7.92937397957\n",
      "The seate state of the mathin the mathin the mathin the mathin the state of the mathin the mathin the mathin the state of the mathin the mathin the state of the mathin the mathin the mathin the mathin the mathin the mathin the state of the mathin the mathin the mathin the mathin the mathin the mathin\n",
      "Iter 399. \n",
      "    Loss 5312.51367188. Time 7.86880397797\n",
      "The proposed and the networks and the networks and the networks and the networks and the networks and the networks and the networks and the networks and the networks and the networks and the networks and the networks and the networks and the networks and the networks and the networks and the networks\n",
      "Iter 439. \n",
      "    Loss 5040.86230469. Time 7.77456498146\n",
      "The and the and the providing the deep neural networks (DNN) and stare-training data for the convergence the convergence the convergence the convergence the convergence the convergence the convergence the convergence the convergence the providing the deep neural networks (DNN) and stare-training data\n",
      "Iter 479. \n",
      "    Loss 4470.01757812. Time 7.60930395126\n",
      "The a simple for the erformation in the such a signification in the such a signification in the such a signification in the such a signification in the such a signification in the such a signification in the such a signification in the such a signification in the such a signification in the such a si\n",
      "Iter 519. \n",
      "    Loss 4618.765625. Time 7.65417814255\n",
      "The and the convex that convex the convex to a deep networks and the convex to a deep networks and the convex to a deep networks and the convex to a deep networks and the convex to a deep networks and the convex to a deep networks and the convex to a deep networks and the convex to a deep networks an\n",
      "Iter 559. \n",
      "    Loss 5174.0703125. Time 7.63376307487\n",
      "The and experiments and the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the seare the sea\n",
      "Iter 599. \n",
      "    Loss 4427.28027344. Time 7.71138596535\n",
      "The form a simple for the such as a semples and state-of-the-art results of the such as a semples and state-of-the-art results of the such as a semples and state-of-the-art results of the such as a semples and state-of-the-art results of the such as a semples and state-of-the-art results of the such \n",
      "Iter 639. \n",
      "    Loss 3942.48095703. Time 7.61375999451\n",
      "The and state of the and state of the and state of the and state of the and state of the and state of the and state of the and state of the and state of the and state of the and state of the and state of the and state of the and state of the and state of the and state of the and state of the and stat\n",
      "Iter 679. \n",
      "    Loss 4141.88574219. Time 7.72997784615\n",
      "The and experimental results on the and experimental results on the and experimental results on the and experimental results on the and experimental results on the and experimental results on the and experimental results on the and experimental results on the and experimental results on the and exper\n",
      "Iter 719. \n",
      "    Loss 3623.16674805. Time 7.74377393723\n",
      "The approach in the interpreted in the networks and a proposed by the approach in the interpreted in the networks and a proposed by the approach in the computation in the computation in the computation in the networks and a proposed by the approach in the interpreted in the networks and a proposed by\n",
      "Iter 759. \n",
      "    Loss 4007.01318359. Time 9.57884597778\n",
      "The and the to the learning and the learning and the learning and the learning and the learning and the learning and the learning and the learning and the learning and the learning and the learning and the learning and the learning and the learning and the learning and the learning and the learning a\n",
      "Iter 799. \n",
      "    Loss 4015.23803711. Time 8.96637701988\n",
      "The reconsing stacking algorithms for the recent results on a stacking of the state-of-the-art results on a stacking of the state-of-the-art results on a stacking of the state-of-the-art results on a stacking of the state-of-the-art results on a stacking of the state-of-the-art results on a stacking \n",
      "Iter 839. \n",
      "    Loss 3539.50317383. Time 10.7214570045\n",
      "The a contress the context of the network architectures that is the interpret that is the interpret that is the interpret that is the interpret that is the interpret that is the interpret that is the interpret that is the interpret that is the interpret that is the interpret that is the interpret tha\n",
      "Iter 879. \n",
      "    Loss 3431.05053711. Time 15.0775768757\n",
      "The and the exploing the distribution from the sparse consider the sparse consider the sparse consider the sparse consider the sparse consider the sparse consider the sparse consider the sparse consider the sparse consider the sparse consider the sparse consider the sparse consider the sparse conside\n",
      "Iter 919. \n",
      "    Loss 3342.76416016. Time 12.0418379307\n",
      "The state-of-the-art deep neural networks (DNNs) as the entiment and data for the success und of a senvident of deep neural networks (DNNs) as the entiment and data for the success und of a senvident of deep neural networks (DNNs) as the entiment and data for the success und of a senvident of deep ne\n",
      "Iter 959. \n",
      "    Loss 3730.55712891. Time 12.1380989552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The achieve an algorithms such as stochastic gradient descent in the computation of the network speed using an approximation on the computation of the computation of the computation of the computation of the computation of the computation of the computation of the computation of the computation of th\n",
      "Iter 999. \n",
      "    Loss 3243.66064453. Time 9.7541949749\n",
      "The accuracy of the accuracy of the accuracy of the accuracy of the accuracy of the accuracy of the accuracy of the accuracy of the accuracy of the accuracy of the accuracy of the accuracy of the accuracy of the accuracy of the accuracy of the accuracy of the accuracy of the accuracy of the accuracy \n",
      "Iter 1039. \n",
      "    Loss 3593.83837891. Time 11.6461241245\n",
      "The and an explored in the networks and a set of the interpretation and the resulting and a set of the interpretation and the resulting and a set of the interpretation and the resulting and a set of the interpretation and the resulting and a set of the interpretation and the resulting and a set of th\n",
      "Iter 1079. \n",
      "    Loss 3222.86279297. Time 14.2522058487\n",
      "These into a single layers of a new to a deep networks are simple and standard neural networks (DNN) is an an and these to and stationary to learn deep networks in a convergence recurrent in the information that is a single learning to a different training of the interfection from the information tha\n",
      "Iter 1119. \n",
      "    Loss 3524.41503906. Time 11.0519380569\n",
      "The state-of-the-art formation of the resulting a group of by a state-of-the-art formation of the resulting a group of by a state-of-the-art formation of the resulting a group of by a state-of-the-art formation of the resulting a group of by a state-of-the-art deep networks (DNNs) as state-of-the-art\n",
      "Iter 1159. \n",
      "    Loss 2971.70751953. Time 11.0589900017\n",
      "The activation finding matrix structure. The contriat to the context of the context of the context of the computation in the compositional can be computed and the reate efficienty computing the computer vision systems, convex optimization problem intoragchity and recond specifies the proposed method \n",
      "Iter 1199. \n",
      "    Loss 3551.97460938. Time 9.191655159\n",
      "The accularian of pooling and the deep learning algorithm is speech recognition (RBM) have been such as the deep learning algorithm is speech recognition (RBM) have been such as the deep learning algorithm is speech recognition (RBM) have been such as the deep learning algorithm is speech recognition\n",
      "Iter 1239. \n",
      "    Loss 3304.12304688. Time 9.09426593781\n",
      "The activation function from chanden learning algorithm is inseves and deep neural networks (DNNs) as an allow deep learning may not features from the state-of-the-art results on a new algorithm as inversign fixed-poond methods whoch erament to developed by the framework we show that the state-of-the\n",
      "Iter 1279. \n",
      "    Loss 3288.31665039. Time 9.57221293449\n",
      "The accuracy of the approach in precently introduce a set of deep networks are able to perform and a set of deep networks are composing the approach in precently introduce a set of deep networks are composing the approach in precently introduce a set of deep networks are composing the approach in pre\n",
      "Iter 1319. \n",
      "    Loss 3249.74169922. Time 9.76282811165\n",
      "The accuracy of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting of the resulting\n",
      "Iter 1359. \n",
      "    Loss 2997.26953125. Time 8.21566700935\n",
      "The functions with the contral for the success of a feature that can recognition in the model in the functions with the contral for the success of a feature that can recognition in the model in the computation in the functions with the contral for the success of a feature that can recognition in the \n",
      "Iter 1399. \n",
      "    Loss 3170.17675781. Time 7.6646668911\n",
      "The accuracy of the non-overgeans of a non-convex optimization of a new approach that compositional and stochastic gradient descent is a deep networks is an accilational neural networks (DNN) model into a single machine learning tasks. We introduces an a distribution of a method compositional and sto\n",
      "Iter 1439. \n",
      "    Loss 3407.73632812. Time 7.79217982292\n",
      "The and functions to the and stated to train and the and calablt model the active and the and calablt model the active and the and calablt model the active and the and calablt model the active and the and calablt model the active and the and calablt model the active and the and calablt model the acti\n",
      "Iter 1479. \n",
      "    Loss 2932.1171875. Time 7.90520787239\n",
      "The activation function that havely are the recognition problem in training deep neural networks (DNNs) as a successful layers to learn to lare between training and interple a standard approaches are gradient descent that the reason to istrocually the success of our methods to achieve that the convex\n",
      "Iter 1519. \n",
      "    Loss 3059.16162109. Time 7.67822098732\n",
      "The activation for the information that interpretable to a large non-convex optimization to minimization to a single layers and related to training to a single learning than object recognition invartaces frames (training of the non-convex optimization problem intoraghine loy-lineline architecture of \n",
      "Iter 1559. \n",
      "    Loss 3015.80004883. Time 7.65474390984\n",
      "The successfully used by a similar our algorithm of the successfully used by a similal out unit (aptwoitstim applied on DNN acturated by a similar our algorithm corverges exporting the state-of-the-art meanune models and state-of-the-art results of a corrent architectures to develop erformations for \n",
      "Iter 1599. \n",
      "    Loss 3041.71728516. Time 7.65510797501\n",
      "The activation function of are compoutues and structured sparearly pooling of deep networks are able to implement dependence of the network proposes a new pre-training and deep neural networks are able to implement dependence of the network propose a non-convex optimization proposes and output shape.\n",
      "Iter 1639. \n",
      "    Loss 2767.69970703. Time 7.9313390255\n",
      "The and explication of the maxout units and estamias of the layers of respect to be relevent of the layers from the layer of the network with different deep networks and random one training deep networks on a deep networks on a new approach for decide networks with different deep networks (DNN) have \n",
      "Iter 1679. \n",
      "    Loss 2938.22363281. Time 7.61386990547\n",
      "The state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on\n",
      "Iter 1719. \n",
      "    Loss 2715.16455078. Time 7.58789491653\n",
      "The conventional neural networks the method computing the conventional conventional layers in a deep neural networks the method computing the conventional conventional layers computational conconvex optimization and computation in the conventional layers that can be about the conventional layers that\n",
      "Iter 1759. \n",
      "    Loss 3009.48339844. Time 7.74926805496\n",
      "The state-of-the-art on a conventional pooling one layers to deep network to provide a convex ERpormances in a deep neural network (DNN) models (training (loggnd MBN in uncuptructive on the state of the art on the state of the successful acoustic log-likelihood operations that representation and stan\n",
      "Iter 1799. \n",
      "    Loss 2711.22827148. Time 7.70024800301\n",
      "The resulting the approach is trained of the network in the network is from a margin learning rules a deep networks trains the correct set of deep neural networks types of our methods to further improvements in the network is for training deep learning algorithms to deco precisient training of the ne\n",
      "Iter 1839. \n",
      "    Loss 2728.02392578. Time 7.6628780365\n",
      "The convergence to a statianifne shown to be a small roustificantly feat with state-of-the-art features from the loss functions is a single machine learning them to a larger convergence to a statianifne structure that are trained on the input functions is computation of the model interaction of the l\n",
      "Iter 1879. \n",
      "    Loss 2882.32714844. Time 7.60884690285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The activation function that the proposed RNN on a classification allows the state-of-the-art models and state-of-the-art models and state-of-the-art models and state-of-the-art minimized to computer vision, we propose a new algorithm is a simple complex one framework to a search for a solvel propaga\n",
      "Iter 1919. \n",
      "    Loss 2708.12744141. Time 7.60294985771\n",
      "The activation function and the approach is an accurate armilation of a neural networks are embedde in order to be the rease in a single madring models that imale compound. We propose a propremsto deep networks are embedded convergence rates of a subcompress the approximations in deep networks are ab\n",
      "Iter 1959. \n",
      "    Loss 2697.81542969. Time 7.71944785118\n",
      "The and the approach for difficulties is to a standard and representation is the main results in a distributed convergence rates in the context of the input sparsity relationship networks is an interpretables scale of the network structure of the norm of the input sparsity relationship networks is an\n",
      "Iter 1999. \n",
      "    Loss 2792.57226562. Time 7.80786204338\n",
      "The recently propose a connections and a similar by a simple and stochastic gradient descent that the distillarization patasetion protopos, and the deep learning algorithms in and model as a features firent seme-theer to the state-of-the-art on the cloas in the core in a deep neural networks (DNN) to\n",
      "Iter 2039. \n",
      "    Loss 2682.56030273. Time 7.83509993553\n",
      "The convong and deep neural networks are bet utilized a standard been layers of deep neural networks are bet utilized a standard been layers of different deep networks in network composed and neural networks that complex models and deep neural networks that complex models and deep neural networks tha\n",
      "Iter 2079. \n",
      "    Loss 2783.46533203. Time 7.74906301498\n",
      "The hidden factor methods several deep neural networks that is in RFNs part there converges in the context of the input data rignist to predict the find generalization for the input data rigntly results on a number of learning and stochastic gradient descent (ising. We show that data approximation fr\n",
      "Iter 2119. \n",
      "    Loss 2549.80639648. Time 7.6661939621\n",
      "The recently proposed models that is ensimple a simpleracts framework in the network is based on the same of the network is essentially effective improted by the function of can be used to the same promoses a set of eximbullow examping large variables are ample a simple machine learning rates for gra\n",
      "Iter 2159. \n",
      "    Loss 2731.67919922. Time 7.74776291847\n",
      "The converges to deep learning algorithms such as deep neural networks to form a converges to advantage that implement deep neural networks to form a convergence recurrent learning that are training of the network depth and these convolutional neural networks with little error or implements in the co\n",
      "Iter 2199. \n",
      "    Loss 2674.43432617. Time 7.65965795517\n",
      "The hidden factors for each layers of a new al of analysis of a deep network constraints and previously thaily linear and an experimental results in the model computation with different distallyneses and a learning different distance of state-of-the-art on the correctly provide a deep network and the\n",
      "Iter 2239. \n",
      "    Loss 2810.90136719. Time 7.98382091522\n",
      "The results and show that the proposed method uses ever with a pre-training and explaried of the results and show that the embedding of the resulting processing settings of the recently proposed method can state of the art on states and the resulting and intent and low-domain the proposed methods com\n",
      "Iter 2279. \n",
      "    Loss 2470.05810547. Time 7.93193483353\n",
      "The context of the interpretable have been such as systems in optimization technique istroduces such as stochastic gradient descent (io the convergence rates of the network size of the network size of the network size of the network size of the network size of the network size of the network size of \n",
      "Iter 2319. \n",
      "    Loss 2866.59033203. Time 8.22401499748\n",
      "The high contral order approach for the interple a similar for training deep learning in a deep learning in a deep learning in a deep learning in a deep learning in a deep learning in a deep learning in a deep learning in a deep learning in a deep learning in a deep learning in a deep learning in a d\n",
      "Iter 2359. \n",
      "    Loss 2526.39746094. Time 7.84498310089\n",
      "The approaches for the input descent input and large DNNs woikel that computation information are a log-efit that the network is a based on a deep neural network architecture that computation information are a log-efithe can be trained discrimitative to be trained by the approaches for the input desc\n",
      "Iter 2399. \n",
      "    Loss 2573.1640625. Time 7.80630397797\n",
      "This paper, we present a new variation of the convergence rates in the convergence rates in the convergence rates in the convergence rates in the convergence rates in the convergence rates in the convergence rates in the convergence rates in the convergence rates in the convergence rates in the conve\n",
      "Iter 2439. \n",
      "    Loss 2750.28515625. Time 7.90565991402\n",
      "The resulting from the unseen deep learning algorithms that is demonstrating from the proposed methods, we find that the network architecture can be mecounieg, the proposed method uses ever with a feature of computational resourt probasification proceds, and the deeper layers, can be employed by the \n",
      "Iter 2479. \n",
      "    Loss 2597.20214844. Time 7.82964110374\n",
      "The convolutional neural networks that can of neural networks that can of neural networks that can of neural networks that can of neural networks that can of neural networks that can of neural networks that can of neural networks that can of neural networks that can of neural networks that can of neu\n",
      "Iter 2519. \n",
      "    Loss 2426.10058594. Time 7.80175900459\n",
      "This paper, we present a novel neural networks (DNN) have standard and output state-of-the-art on a different distributions. We evaluated on a different distributions. We evaluated for supervised wide results on a different distributions. We evaluated for supervised wide results on a different distri\n",
      "Iter 2559. \n",
      "    Loss 2272.27954102. Time 7.88422513008\n",
      "The success of deep neural networks at recognition of the recently proposed maxout units and requires from the network in the same our networks at recently proposed maxout unit. The submodel for the training of the art on the structurel-time. Hese them to the success of deep neural networks (DNNs) an\n",
      "Iter 2599. \n",
      "    Loss 2554.33129883. Time 7.81755208969\n",
      "The context of the importance of network depth and intermed applies consisted of the interper the network dependegayer the method for convergence rates for general models to a contrast the convergence rates in the content computational content from speech and investigate the context of the network de\n",
      "Iter 2639. \n",
      "    Loss 2468.36206055. Time 8.13583707809\n",
      "The activation ensiminally to a search for a good generators a new approach to a single layer to lated information and stacked RNNs output recurrent neural network (DNN) as machine learning tasks. To sopritze  well as method is significant unseen in the emperiments like on the function at test the no\n",
      "Iter 2679. \n",
      "    Loss 2355.89624023. Time 7.80260896683\n",
      "The state-of-the-art set of examples to predict the conventional recognition to the space of learning algorithm for the correct process one machine learning the computation in the results shown that data. The learned standat, a deep networks that estration a control ess the model that are so faith tr\n",
      "Iter 2719. \n",
      "    Loss 2299.31640625. Time 7.81513905525\n",
      "The hidden factors for existing respect to for an approximation of the input specifled by architecture problems. With learning than particular, on these for each layers of our precessence transfer been successfully and straight-through estimation of the model is straight-through well as these for eac\n",
      "Iter 2759. \n",
      "    Loss 2228.29052734. Time 7.78356599808\n",
      "The recently propost the successful to train a single network by a simple experiments on a mechanism to better initialize the first objective the final generally compared to train the demoht (GD) hidden unit (senvident only 16-bboled decoder. The highin a connection with the input of the training of \n",
      "Iter 2799. \n",
      "    Loss 2242.49316406. Time 7.91537189484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper provides the parallelism. We replication and computational capacity. We develop a topouthys to the output data and fully are component of parameters that deep neural networks to be a deep neural networks to be a deep neural networks (DNNs) a BL-12 that have been their can be used to transf\n",
      "Iter 2839. \n",
      "    Loss 2521.32641602. Time 7.91025900841\n",
      "This paper, we present a new that the first state-of-the-art on compared to analyze the algorithm is in a simple structure. This algorithms to learn a convex optimization problems with the input of the source domain adaptation, in the training parameters, and the training of the input stacked Resk mo\n",
      "Iter 2879. \n",
      "    Loss 2283.21606445. Time 7.98815202713\n",
      "The successive is competitive with state-of-the-art or biofertional results on a subset sampling algorithms to accurately effective infiked for functions that representations of the recently proposed $L_p$ unit on the same training of computations and state-of-the-art performance on depth for new pra\n",
      "Iter 2919. \n",
      "    Loss 2395.86303711. Time 8.01282906532\n",
      "This paper preservation achieved a new training and deep neural networks than even problems of multiple different deep neural networks than even problems of multiple different deep neural networks than even problems of multiple different deep neural networks than even problems of multiple different d\n",
      "Iter 2959. \n",
      "    Loss 2360.95947266. Time 7.91659498215\n",
      "The hidden factors for each layer of standard and achieve this distill in a single level a small rognitional networks (DNN) have learning and the denited to the deep learning methods to achieve the data to full we show that the nodes of the layer neural network architecture can be seliftly in a singl\n",
      "Iter 2999. \n",
      "    Loss 1793.82861328. Time 8.92366886139\n",
      "The success that the proposed maxout networks, as well as the recurrent neural networks (DNNs) as results in the network. Specificalizes state-of-the-art results on a multing the transformation, each neurons of a method to formation of the recently proposed maxout units and require discribited to lea\n",
      "Iter 3039. \n",
      "    Loss 2342.55981445. Time 8.27429199219\n",
      "The control of the selection in the context of the methods of convergence rates of the input. Onevel of the input for learning is a single from models in the accuracy of the main results on a different deep neural networks than emorobilize the accuracy rates of rectifier convergation error on the con\n",
      "Iter 3079. \n",
      "    Loss 2438.90307617. Time 8.18312191963\n",
      "The gradient of the maximum independent of the training of the algorithm for continuous speed us a recult in the earit time set of descons. Our experimental resulting in the best of second successfully trained using a classification performance and model as the encoder and decoder can be trained usin\n",
      "Iter 3119. \n",
      "    Loss 2094.3737793. Time 7.92329096794\n",
      "The explicit computation in the conventional layer-wise proposed deep neural networks. The proposed method can be used to be the output of the network architectures. These important to implement for the input factors. The proposed maximal framework for understanding the approach is a large number of \n",
      "Iter 3159. \n",
      "    Loss 2397.36450195. Time 7.59778904915\n",
      "The hidden factors for evaluated by the loss functions of a neural networks that is its is versally in parameters and the data distributions. We propose a novel applications, and the convergence rates of the standard approach to a single from more and stacked Recturied (SBM) and releved as a standard\n",
      "Iter 3199. \n",
      "    Loss 2397.65087891. Time 7.70139908791\n",
      "The recently proposed descespents in significantly improves provided a gradient architecture of orbits and state of the art on state-of-the-art onfers to train may be modeling longly higher-order regresers and show that itsignist seatises are event proposed descent also erpor measure of deep networks\n",
      "Iter 3239. \n",
      "    Loss 2338.64208984. Time 7.91256213188\n",
      "This paper previous data to the network depth and networks and previously the model interpretation to a non-negative methods. The proposed mathom with the interp methons in contrasNs that the pre-training algorithms that match or ourpersorm alto it is a temmory, we demonstrate the pre-training algori\n",
      "Iter 3279. \n",
      "    Loss 2240.29321289. Time 7.98480916023\n",
      "The CIF parametrization problems (GL-RFNs resigning. In this paper, we emplor a large number of benchmarks. Ginenting of the concepts of an MDRNN is and relative to analyze a propasy an aptracs to the training conditions and random decations, and we have been used for Quantitate the encoded in the en\n",
      "Iter 3319. \n",
      "    Loss 2266.17919922. Time 7.75929689407\n",
      "The success of deep neural networks (DNNs) as unsupervised learning algorithms as the embeddings formetribee to maximization that is not essyit called method using the componing initial results suggest that fuld methods for the training arg inamelication of the commonly and compared to the tracked re\n",
      "Iter 3359. \n",
      "    Loss 2144.54956055. Time 7.87121605873\n",
      "This paper provabling, which estem is the method of parameters and structural therrouruly the network computation of the main result is computation of the main result is bask on empirical in parrexpression find approximation of the main result is an about tenine improvements in the computation of the\n",
      "Iter 3399. \n",
      "    Loss 2202.30175781. Time 7.8241519928\n",
      "The resulting our algorithm is insight modectave ond the toper architecture that is the training and estemsis a large number of benchmark datasets such as the context of the input of the norm experiments show that the proposed deschip contribution for analysis and different distribution than hieder t\n",
      "Iter 3439. \n",
      "    Loss 2097.55859375. Time 7.76153302193\n",
      "The existing learning rate and a recurrent neural networks (DNNs) as a successing a formal convergence properties are general molecelements. We show that the proposed model by use of the configuest our methods to implement from supporting that the sequence are also actively simple linear activities t\n",
      "Iter 3479. \n",
      "    Loss 2148.046875. Time 7.92079997063\n",
      "The hidden factors for evelt of parameters of a standard based on the standard and output stacked RNN by relations in which the convex optimization of embedded them to a formalizen standard nonsof it is usibiel of the convergence rates of the input fact, whosh errect of an experiments sevartamelicrar\n",
      "Iter 3519. \n",
      "    Loss 2251.67480469. Time 7.79490804672\n",
      "The recently proposed descent and deep networks in the training and experimental results on the number of benchmark datasets that can use the continion results on the prediction be used in speed us a propose a new approach a deep connicuented to method for desing deep learning algorithms that can be \n",
      "Iter 3559. \n",
      "    Loss 2178.40673828. Time 7.8216149807\n",
      "This paper provides the activation function and training particuaring and data augmentation. We introduce a new approach to a machine learning tasks the pooling operation of a model by paranenterization problems with the input factor of the network are able to model structure to the non-convex optimi\n",
      "Iter 3599. \n",
      "    Loss 2153.25024414. Time 7.80773711205\n",
      "The variance reduction that is not using gradient descent (SGD); but their theoretical analysis are sped and sparser models using gradient descent (SGD); but their theoretical analysis are sped and sparser models using gradient descent (SGD); but their theoretical analysis are sped and sparser models\n",
      "Iter 3639. \n",
      "    Loss 2150.07861328. Time 7.75642085075\n",
      "The successive in the recently propose a new approach for crase of the successive in the recently propose a new approach for crase of the successive in a recurrent neural network (DNN) to the successing a new approach for convensing neuronstion with a similar strategy to the output of the success of \n",
      "Iter 3679. \n",
      "    Loss 2261.47827148. Time 7.78062391281\n",
      "This paper predicts with models. Although neural networks than computer vision to the pre-training matrix scap new neural networks. We find that a simple scognder transfer speedup with a computational and transposed convolutional neural networks (DNNs) as deep network. This mapp onitionally introduce\n",
      "Iter 3719. \n",
      "    Loss 2171.42407227. Time 7.79096484184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The high prediction accuracy of MBN but learn that the now-deer $L_p$ unit is in compatible with state-of-the-art DNN and model by a deep network consisting of deep learning in deep learning making them input-space solver interpretation of the DNNs is a single model to several introduced and differen\n",
      "Iter 3759. \n",
      "    Loss 2255.68994141. Time 7.92971897125\n",
      "The effect of a finilg not of the existing research on knowledge to employ derived using HF transformation, and show that backpropagations, and show that backpropagations to a large model as multiple mapsifoc) computational language for acoustic models can be about the effect of a finilg a pooling op\n",
      "Iter 3799. \n",
      "    Loss 2223.47851562. Time 7.89682507515\n",
      "The convex optimization in this context a formal convergence the standard Spreveral neural networks with convolutional neural networks to speech effortor error computations for supervised pretraining on computation deep neural networks are able to perfort but also forwand pooling operations, and a li\n",
      "Iter 3839. \n",
      "    Loss 2107.71313477. Time 7.84011602402\n",
      "The resulting the proposed RNN outperforms a deep network consisting researchers on the nodes of converse clustering and exploding gradient descent (SGD), in the propost linear and manifoll-ounclasions of the RNN model to the overall compression error on comparameter to learn the parameters and the d\n",
      "Iter 3879. \n",
      "    Loss 2188.296875. Time 7.97986793518\n",
      "This paper proposes a context can be active/inacter decoders, and then embedd to the success of the relationship by a model by the advantage of large constrained architectures based on an approximately learn in the learning algorithms activation function of the network. We present a new pre-training \n",
      "Iter 3919. \n",
      "    Loss 2116.76611328. Time 8.02697610855\n",
      "The gains of neural networks with convexity of CTC--or successful the training examples. We introduce a new applied to a large number of parameters for several invariant on standard but allo1 of convergence rates for the data droted to the artanismoncal set of the subject is a blokk repornes with the\n",
      "Iter 3959. \n",
      "    Loss 2296.36621094. Time 8.00406193733\n",
      "The effectiveness of our propose a new architectures can be assigned by establishing a new architectures in the parallelism of the recently proposed descent and time compression to maximally preserving a gradient (ectived a propose a new architecture and the essential processing to extract the state \n",
      "Iter 3999. \n",
      "    Loss 2155.20263672. Time 8.10965299606\n",
      "This paper presents a new representation are models into a single machine learning mask in a deep neural networks (DNNs) as deep neural networks. The proposed maxoming matrix and interpretad as a new pre-training and data studabition of deep neural networks. The proposed maxoming matrix and interpret\n",
      "Iter 4039. \n",
      "    Loss 2110.6796875. Time 7.88979792595\n",
      "The layer better nodes of difference and model complex to latenorget increasing and different distributions. We propose a new system complex to a deep learning problem involving a general matrix framework for a group of muluiled data of sperif can be simple context of different distributions. We prop\n",
      "Iter 4079. \n",
      "    Loss 2172.81396484. Time 8.17105579376\n",
      "The existence of the success of deep neural networks that is ins, a low-domated that is trieg veace loss functions and require that the state-of-the-art or bits infirement first-order information that can be reduced by a superior discespeeds that our approach outperforms the success of our method usi\n",
      "Iter 4119. \n",
      "    Loss 2268.8918457. Time 8.03650784492\n",
      "This paper previous data transformations, into discriment simple learning propesties method with a computation exponentially achieve thes, in a theoretical accularity of neural networks with more effectiveness of feature from the pooling to learn an encodert based on the computation of convolutional \n",
      "Iter 4159. \n",
      "    Loss 2060.35009766. Time 8.12586092949\n",
      "The value function approximation of the prediction by a deep connicuence and for the training (inition processing and computation designification are graphion and insights getent and compared to an ergor racred distribution frameworks. We inspired by the individual results on a large number of regula\n",
      "Iter 4199. \n",
      "    Loss 2021.6126709. Time 8.0652859211\n",
      "The effectiveness of our experiments show that for both a convexp networks to build random naithis (single sound to train and the encoder and deep neural networks. We firing the system on embedded systems, out neural networks. We form a constrained particlising the system to the state-of-the-art resu\n",
      "Iter 4239. \n",
      "    Loss 2092.22875977. Time 8.89707612991\n",
      "The hidden factorization of the and computation efficient machine learning thematically, the activation functions, we show that deep neural networks with outputs and output some DNN output for an optimal capacity respent in a cluster of functions, we train a general faster processing and entrably to \n",
      "Iter 4279. \n",
      "    Loss 2032.10644531. Time 8.38225698471\n",
      "The allows to build decoders are relevantly employs layers to date on the optimization generators wive network without alled as analyze that this adorkp weights learned as a model. We also introduce a set of experiments limited to generate somithm by learned by the DNNs using supervised learned GProp\n",
      "Iter 4319. \n",
      "    Loss 2004.52160645. Time 8.6063811779\n",
      "This paper propose a novel about the generative mithor constrained as the learning algorithms to the state-of-the-art predicces for analysis of machine learning tasks. However, of iterations to the postible trained on the state of the art speech recognition tasks on the computation of convergantions \n",
      "Iter 4359. \n",
      "    Loss 1973.04089355. Time 8.60600018501\n",
      "The variationons on the standard and achieves an ever more spack to an entable by a number of convergence rates of convex optimization, and is the subjeet the first only distribeted in the intermediate representation are not for DBN the normalization and state-of-the-art methods of points --  widely \n",
      "Iter 4399. \n",
      "    Loss 1948.19335938. Time 8.65848398209\n",
      "The resulting network acoustic mbetive as a parameter can be a single machines encighe large structure, DBNs have been reported in analyze to train and lorge that makes this paper, we propose a novel regule generative model for the input of the network depth and stacked RNNs on the state-of-the-art r\n",
      "Iter 4439. \n",
      "    Loss 1986.74475098. Time 8.41595292091\n",
      "This paper previous dopaint models and maxout unit (e. While a computation exponentially activation function and successes and neural networks are able to pre-train very previously unleree using and data that a new regularizing data uprated of the network in the content achieved a new pre-training ma\n",
      "Iter 4479. \n",
      "    Loss 2042.6003418. Time 8.10441398621\n",
      "The resulting a group activation from the domain respect to the output of the algorithm for convex ERMs and standard CIn of the and constraints at setting of the proposed descriptor learning task, in a stacking function to the input. Which learns than those encoder and factor giousance in order to be\n",
      "Iter 4519. \n",
      "    Loss 1926.75976562. Time 7.91439795494\n",
      "The effect of learning the control of the recently proposed deeper-linear algorithm for fundate produced. We also present a new speed, by issues in the recently proposed deepery, Deep networks that isently incrementation lysimpentially persop weas the embeddings factorizations and results on a cluste\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
